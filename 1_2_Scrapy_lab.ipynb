{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjqGtEqMUB2O"
      },
      "source": [
        "<img src=\"https://www.uc3m.es/ss/Satellite?blobcol=urldata&blobkey=id&blobtable=MungoBlobs&blobwhere=1371573952659\">\n",
        "\n",
        "---\n",
        "\n",
        "# WEB ANALYTICS COURSE 4 - SEMESTER 2\n",
        "# BACHELOR IN DATA SCIENCE AND ENGINEERING\n",
        "\n",
        "# LAB 1.2 WEB SCRAPING WITH SCRAPY\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Marina Gómez Rey (100472836)\n",
        "### María Ángeles Magro Garrote (100472867)\n",
        "### Sara Piñas García (100472784)\n",
        "### Eduardo González Agüero (100472704)"
      ],
      "metadata": {
        "id": "pjvyTVuAzony"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS1O4BW82LV6"
      },
      "source": [
        "# 0. Lab Preparation\n",
        "\n",
        "1.  Study and have clear the concepts explained in the theoretical class and the introductory lab.\n",
        "\n",
        "2.   Gain experience with the use of the [Scrapy](https://scrapy.org/). The exercises of this lab will be mainly based on the utilization of functions offered by this library.\n",
        "\n",
        "3. It is assumed students have experience in using Python notebooks. Either a local installation (e.g., local python installation + Jupyter) or a cloud-based solution (e.g., Google Colab). *We recommend the second option*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwlGXDUG2db2"
      },
      "source": [
        "# 1. Lab Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypNv8Fpi2YS7"
      },
      "source": [
        "* In this lab, we will implement a web scraper using [Scrapy](https://scrapy.org/). One of the tools explained in the theoretical class.\n",
        "\n",
        "* The lab will be done in groups of 4 people.\n",
        "\n",
        "* The lab defines a set of milestones the students must complete. Upon completing all the milestones, students should call the professor, who will check the correctness of the solution (*If the professor is busy, do not wait for them, move to the next lab*).\n",
        "\n",
        "* **The final mark will be computed as a function of the number of milestones successfully completed.**\n",
        "\n",
        "* **Each group should also share their lab notebook with the professor upon the finalization of the lab.**\n",
        "\n",
        "* In this lab we will use the [Scrapy](https://scrapy.org/) library for the creation of a web scraper, to extract information from the web. As indicated in the *Lab Preparation* section above, it is expected that students have gained experience in the use of the library before starting the first session of the lab.\n",
        "\n",
        "- It is recommended to use [Google Colab](https://colab.research.google.com/) to produce the Python notebook with the solution of the lab. Of course, if any student prefers using its local programming environment (e.g., jupyter) and python installation, they are welcome to do so."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnIUv5WsKSfJ"
      },
      "source": [
        "%%capture\n",
        "!!pip install scrapy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhPF9V3_Jyjv"
      },
      "source": [
        "# MILESTONE 1\n",
        "\n",
        "a) Create a _Scrapy_ project for this lab (**HINT:** Remember you can use the command `startproject`).\n",
        "\n",
        "b) Create a crawler/spider to the website [BACHELOR IN DATA SCIENCE AND ENGINEERING\n",
        "](https://www.uc3m.es/bachelor-degree/data-science).\n",
        "\n",
        "c) Add the code to the crawler to get PROGRAM header. **TIP:** Find the element tag with `id=\"program\"` and print the result.\n",
        "\n",
        "d) Add the code to the crawler to find the table inside PROGRAM for Course 1 - Semester 1 and print the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR58vq8AwZuQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "588e7444-17be-4f83-bba0-55126b274744"
      },
      "source": [
        "%mkdir \"Lab2_scrapy\"\n",
        "%cd \"Lab2_scrapy\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Lab2_scrapy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcpQL0jgXBPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6a80731-aaa4-45fd-ff83-9ce00295ef47"
      },
      "source": [
        "import scrapy\n",
        "!scrapy startproject lab_scrapy_1_2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Scrapy project 'lab_scrapy_1_2', using template directory '/usr/local/lib/python3.10/dist-packages/scrapy/templates/project', created in:\n",
            "    /content/Lab2_scrapy/lab_scrapy_1_2\n",
            "\n",
            "You can start your first spider with:\n",
            "    cd lab_scrapy_1_2\n",
            "    scrapy genspider example example.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkBV7gekXBMB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99eb97d4-522e-4f10-8fc7-88684a9e204d"
      },
      "source": [
        "%cd \"lab_scrapy_1_2\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Lab2_scrapy/lab_scrapy_1_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy genspider data_spider https://www.uc3m.es/bachelor-degree/data-science"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHgVOhsq1a45",
        "outputId": "e8f28ab2-f80c-4477-a499-5ebcebba18e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created spider 'data_spider' using template 'basic' in module:\n",
            "  lab_scrapy_1_2.spiders.data_spider\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    def parse(self, response):\n",
        "        print(response.xpath(\"//*[contains(text(),'Program')]\").get())\n"
      ],
      "metadata": {
        "id": "fme0QGnt2U_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy crawl data_spider"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUC5Kwlp1oSt",
        "outputId": "b5ed86c6-5d74-429b-c5a6-cf260644e1a4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-25 09:12:07 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: lab_scrapy_1_2)\n",
            "2024-09-25 09:12:07 [scrapy.utils.log] INFO: Versions: lxml 4.9.4.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.1, Platform Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "2024-09-25 09:12:07 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2024-09-25 09:12:07 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2024-09-25 09:12:07 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2024-09-25 09:12:07 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2024-09-25 09:12:07 [scrapy.extensions.telnet] INFO: Telnet Password: 53abfa806a8e78d5\n",
            "2024-09-25 09:12:07 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2024-09-25 09:12:07 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'lab_scrapy_1_2',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'lab_scrapy_1_2.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['lab_scrapy_1_2.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2024-09-25 09:12:07 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2024-09-25 09:12:07 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2024-09-25 09:12:07 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2024-09-25 09:12:07 [scrapy.core.engine] INFO: Spider opened\n",
            "2024-09-25 09:12:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2024-09-25 09:12:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2024-09-25 09:12:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/robots.txt> (referer: None)\n",
            "2024-09-25 09:12:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/bachelor-degree/data-science> (referer: None)\n",
            "<h2>Program</h2>\n",
            "2024-09-25 09:12:09 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2024-09-25 09:12:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 462,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 18342,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'elapsed_time_seconds': 1.388195,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2024, 9, 25, 9, 12, 9, 277625, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 92807,\n",
            " 'httpcompression/response_count': 2,\n",
            " 'log_count/DEBUG': 5,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 126930944,\n",
            " 'memusage/startup': 126930944,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2024, 9, 25, 9, 12, 7, 889430, tzinfo=datetime.timezone.utc)}\n",
            "2024-09-25 09:12:09 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "    def parse(self, response):\n",
        "        program_section = response.xpath('.//h3[contains(text(), \"Year 1 - Semester 1\")]/following-sibling::table//tr')\n",
        "\n",
        "        for row in program_section:\n",
        "            cells = row.xpath('.//td//text()').getall()\n",
        "            print(cells)"
      ],
      "metadata": {
        "id": "i3_sFxGperXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy crawl data_spider"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2lyotDdeqt1",
        "outputId": "9a36f0c5-db90-4ca1-b0cd-6db0f9325163"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-25 09:38:47 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: lab_scrapy_1_2)\n",
            "2024-09-25 09:38:47 [scrapy.utils.log] INFO: Versions: lxml 4.9.4.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.1, Platform Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "2024-09-25 09:38:47 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2024-09-25 09:38:47 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2024-09-25 09:38:47 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2024-09-25 09:38:47 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2024-09-25 09:38:47 [scrapy.extensions.telnet] INFO: Telnet Password: 594e27fef50a0df0\n",
            "2024-09-25 09:38:47 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2024-09-25 09:38:47 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'lab_scrapy_1_2',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'lab_scrapy_1_2.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['lab_scrapy_1_2.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2024-09-25 09:38:47 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2024-09-25 09:38:47 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2024-09-25 09:38:47 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2024-09-25 09:38:47 [scrapy.core.engine] INFO: Spider opened\n",
            "2024-09-25 09:38:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2024-09-25 09:38:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2024-09-25 09:38:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/robots.txt> (referer: None)\n",
            "2024-09-25 09:38:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/bachelor-degree/data-science> (referer: None)\n",
            "[]\n",
            "['Calculus I', '6', 'BC']\n",
            "['Introduction to Data Science', '6', 'BC']\n",
            "['Linear algebra', '6', 'BC']\n",
            "['Probability and Data Analysis', '6', 'BC']\n",
            "['Programming', '6', 'BC']\n",
            "2024-09-25 09:38:49 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2024-09-25 09:38:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 462,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 18263,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'elapsed_time_seconds': 1.48156,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2024, 9, 25, 9, 38, 49, 371382, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 92807,\n",
            " 'httpcompression/response_count': 2,\n",
            " 'log_count/DEBUG': 5,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 127201280,\n",
            " 'memusage/startup': 127201280,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2024, 9, 25, 9, 38, 47, 889822, tzinfo=datetime.timezone.utc)}\n",
            "2024-09-25 09:38:49 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHKtqTS95RSx"
      },
      "source": [
        "# MILESTONE 2\n",
        "\n",
        "a) Obtain the link to Web Analytics course by finding the corresponding href.\n",
        "\n",
        "b) Create a new spider _class_ and access to this URL.\n",
        "\n",
        "**TIP**: For this milestone, you need to create a new crawler and give it a different name.\n",
        "\n",
        "c) Print the text inside the _Description of contents: programme_ section.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    def parse(self, response):\n",
        "        web_analytics = response.xpath('//a[span[contains(text(), \"Web Analytics\")]]/@href').get()\n",
        "        print(web_analytics)"
      ],
      "metadata": {
        "id": "UnxS9ah8lBnt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7xkCvEVXDT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebc446fb-03ab-4761-ce9f-521d74a61655"
      },
      "source": [
        "!scrapy crawl data_spider"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-25 09:46:58 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: lab_scrapy_1_2)\n",
            "2024-09-25 09:46:58 [scrapy.utils.log] INFO: Versions: lxml 4.9.4.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.1, Platform Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "2024-09-25 09:46:58 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2024-09-25 09:46:58 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2024-09-25 09:46:58 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2024-09-25 09:46:58 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2024-09-25 09:46:58 [scrapy.extensions.telnet] INFO: Telnet Password: f7ba20e200c93849\n",
            "2024-09-25 09:46:58 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2024-09-25 09:46:58 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'lab_scrapy_1_2',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'lab_scrapy_1_2.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['lab_scrapy_1_2.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2024-09-25 09:46:58 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2024-09-25 09:46:58 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2024-09-25 09:46:58 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2024-09-25 09:46:58 [scrapy.core.engine] INFO: Spider opened\n",
            "2024-09-25 09:46:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2024-09-25 09:46:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2024-09-25 09:46:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/robots.txt> (referer: None)\n",
            "2024-09-25 09:46:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/bachelor-degree/data-science> (referer: None)\n",
            "https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2\n",
            "2024-09-25 09:46:59 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2024-09-25 09:46:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 462,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 18336,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'elapsed_time_seconds': 1.224711,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2024, 9, 25, 9, 46, 59, 567836, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 92807,\n",
            " 'httpcompression/response_count': 2,\n",
            " 'log_count/DEBUG': 5,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 127201280,\n",
            " 'memusage/startup': 127201280,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2024, 9, 25, 9, 46, 58, 343125, tzinfo=datetime.timezone.utc)}\n",
            "2024-09-25 09:46:59 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-yQti1TXDPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de071f8c-2bb0-4556-f9ac-1e144123942e"
      },
      "source": [
        "!scrapy genspider web_spider https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created spider 'web_spider' using template 'basic' in module:\n",
            "  lab_scrapy_1_2.spiders.web_spider\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtEwewGQXDNb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9b7976b-6e97-4bc1-9743-d6fe5af8dee3"
      },
      "source": [
        "!scrapy crawl web_spider"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-25 09:49:22 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: lab_scrapy_1_2)\n",
            "2024-09-25 09:49:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.4.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.1, Platform Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "2024-09-25 09:49:22 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2024-09-25 09:49:22 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2024-09-25 09:49:22 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2024-09-25 09:49:22 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2024-09-25 09:49:22 [scrapy.extensions.telnet] INFO: Telnet Password: e3f1fdeae2a7153f\n",
            "2024-09-25 09:49:22 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2024-09-25 09:49:22 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'lab_scrapy_1_2',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'lab_scrapy_1_2.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['lab_scrapy_1_2.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2024-09-25 09:49:22 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2024-09-25 09:49:22 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2024-09-25 09:49:22 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2024-09-25 09:49:22 [scrapy.core.engine] INFO: Spider opened\n",
            "2024-09-25 09:49:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2024-09-25 09:49:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2024-09-25 09:49:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://aplicaciones.uc3m.es/robots.txt> (referer: None)\n",
            "2024-09-25 09:49:22 [scrapy.core.engine] DEBUG: Crawled (400) <GET https://aplicaciones.uc3m.es/cpa/generaFicha> (referer: None)\n",
            "2024-09-25 09:49:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <400 https://aplicaciones.uc3m.es/cpa/generaFicha>: HTTP status code is not handled or not allowed\n",
            "2024-09-25 09:49:23 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2024-09-25 09:49:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 467,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 1313,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'downloader/response_status_count/400': 1,\n",
            " 'elapsed_time_seconds': 0.492451,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2024, 9, 25, 9, 49, 23, 28690, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 122,\n",
            " 'httpcompression/response_count': 1,\n",
            " 'httperror/response_ignored_count': 1,\n",
            " 'httperror/response_ignored_status_count/400': 1,\n",
            " 'log_count/DEBUG': 5,\n",
            " 'log_count/INFO': 11,\n",
            " 'memusage/max': 140398592,\n",
            " 'memusage/startup': 140398592,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2024, 9, 25, 9, 49, 22, 536239, tzinfo=datetime.timezone.utc)}\n",
            "2024-09-25 09:49:23 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "    def parse(self, response):\n",
        "        programme = response.xpath('.//*[contains(text(), \"Description of contents: programme\")]/following-sibling::div').xpath('string(.)').get()\n",
        "        print(programme)"
      ],
      "metadata": {
        "id": "JQWIWF8upKOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy crawl web_spider"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDKo5VMsmoEd",
        "outputId": "fbd68d56-983b-4e51-e25f-227dbbc31d9e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-25 10:35:48 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: lab_scrapy_1_2)\n",
            "2024-09-25 10:35:48 [scrapy.utils.log] INFO: Versions: lxml 4.9.4.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.1, Platform Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "2024-09-25 10:35:48 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2024-09-25 10:35:48 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2024-09-25 10:35:48 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2024-09-25 10:35:48 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2024-09-25 10:35:48 [scrapy.extensions.telnet] INFO: Telnet Password: b49c57a7345fb6fb\n",
            "2024-09-25 10:35:48 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2024-09-25 10:35:48 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'lab_scrapy_1_2',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'lab_scrapy_1_2.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['lab_scrapy_1_2.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2024-09-25 10:35:48 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2024-09-25 10:35:48 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2024-09-25 10:35:48 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2024-09-25 10:35:48 [scrapy.core.engine] INFO: Spider opened\n",
            "2024-09-25 10:35:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2024-09-25 10:35:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2024-09-25 10:35:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://aplicaciones.uc3m.es/robots.txt> (referer: None)\n",
            "2024-09-25 10:35:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2> (referer: None)\n",
            "\n",
            "          1.\tData Collection in the Web ecosystem: \n",
            "        1.1  Scrapers, Crawlers\n",
            "        1.2\tAPIs\n",
            "2.\tData Analytics in the web\n",
            "        2.1  Graph Analysis: Centrality and Influence metrics\n",
            "        2.2  Network structure: \n",
            "               2.2.1 Type of networks (bipartite graph, small world, scale free)\n",
            "               2.2.2 Clustering, Community Detection, K-core decomposition\n",
            "\n",
            "3.\tWeb data visualization\n",
            "        3.1\tRepresentation of web information.\n",
            "        3.2\tVisualization tools.\n",
            "\n",
            "4.\tFinal Web Analytics Project\n",
            "        4.1\tThe project needs to include the three components presented above (Data Collection, Data Analytics and Data Visualization\n",
            "\n",
            "        \n",
            "2024-09-25 10:35:49 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2024-09-25 10:35:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 505,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 8089,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'elapsed_time_seconds': 1.188342,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2024, 9, 25, 10, 35, 49, 750706, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 17988,\n",
            " 'httpcompression/response_count': 2,\n",
            " 'log_count/DEBUG': 5,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 149581824,\n",
            " 'memusage/startup': 149581824,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2024, 9, 25, 10, 35, 48, 562364, tzinfo=datetime.timezone.utc)}\n",
            "2024-09-25 10:35:49 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN4L9SxRP3Hj"
      },
      "source": [
        "# MILESTONE 3\n",
        "\n",
        "a) Modify your code from previous milestones for running both crawlers in the same command line process.\n",
        "\n",
        "\n",
        "b) Instead of printing the results (from Milestone 1 and 2), save them in a file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G84E8DrUXFwt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c832ea6d-61c0-4dbb-c75b-47ce5db54bef"
      },
      "source": [
        "!scrapy genspider total_spider https://www.uc3m.es/"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spider 'total_spider' already exists in module:\n",
            "  lab_scrapy_1_2.spiders.total_spider\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from scrapy.utils.project import get_project_settings\n",
        "\n",
        "class BachelorSpider(scrapy.Spider):\n",
        "    name = \"bachelor_spider\"\n",
        "    allowed_domains = [\"www.uc3m.es\"]\n",
        "    start_urls = [\"https://www.uc3m.es/bachelor-degree/data-science\"]\n",
        "\n",
        "    def parse(self, response):\n",
        "        program_section = response.xpath('.//h3[contains(text(), \"Year 1 - Semester 1\")]/following-sibling::table//tr')\n",
        "        results = []\n",
        "\n",
        "        for row in program_section:\n",
        "            cells = row.xpath('.//td//text()').getall()\n",
        "            results.append(' '.join(cells))\n",
        "        \n",
        "        with open('results.txt', 'a') as f:\n",
        "            for result in results:\n",
        "                f.write(result + '\\n')\n",
        "\n",
        "class WebSpider(scrapy.Spider):\n",
        "    name = \"web_spider\"\n",
        "    allowed_domains = [\"aplicaciones.uc3m.es\"]\n",
        "    start_urls = [\"https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2\"]\n",
        "\n",
        "    def parse(self, response):\n",
        "        programme = response.xpath('.//*[contains(text(), \"Description of contents: programme\")]/following-sibling::div').xpath('string(.)').get()\n",
        "        \n",
        "        with open('results.txt', 'a') as f:  \n",
        "            f.write(f\"Programme Description: {programme}\\n\")  \n",
        "\n",
        "settings = get_project_settings()\n",
        "process = CrawlerProcess(settings)\n",
        "process.crawl(BachelorSpider)\n",
        "process.crawl(WebSpider)\n",
        "process.start()"
      ],
      "metadata": {
        "id": "WTEwLT1JzWOS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWzVuLtSXFo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "929d52bc-6cad-47ab-b161-c15043557284"
      },
      "source": [
        "!scrapy crawl total_spider"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scrapy/spiderloader.py:49: UserWarning: There are several spiders with the same name:\n",
            "\n",
            "  WebSpider named 'web_spider' (in lab_scrapy_1_2.spiders.total_spider)\n",
            "\n",
            "  WebSpiderSpider named 'web_spider' (in lab_scrapy_1_2.spiders.web_spider)\n",
            "\n",
            "  This can cause unexpected behavior.\n",
            "  warnings.warn(\n",
            "2024-09-25 10:38:39 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: lab_scrapy_1_2)\n",
            "2024-09-25 10:38:39 [scrapy.utils.log] INFO: Versions: lxml 4.9.4.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.1, Platform Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "2024-09-25 10:38:39 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2024-09-25 10:38:39 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2024-09-25 10:38:39 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2024-09-25 10:38:39 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2024-09-25 10:38:39 [scrapy.extensions.telnet] INFO: Telnet Password: 68b4bbb44e3ff711\n",
            "2024-09-25 10:38:39 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2024-09-25 10:38:39 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'lab_scrapy_1_2',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'lab_scrapy_1_2.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['lab_scrapy_1_2.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2024-09-25 10:38:39 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2024-09-25 10:38:39 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2024-09-25 10:38:39 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2024-09-25 10:38:39 [scrapy.core.engine] INFO: Spider opened\n",
            "2024-09-25 10:38:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2024-09-25 10:38:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2024-09-25 10:38:39 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2024-09-25 10:38:39 [scrapy.extensions.telnet] INFO: Telnet Password: 719e82984bda08e9\n",
            "2024-09-25 10:38:39 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2024-09-25 10:38:39 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'lab_scrapy_1_2',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'lab_scrapy_1_2.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['lab_scrapy_1_2.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2024-09-25 10:38:39 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2024-09-25 10:38:39 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2024-09-25 10:38:39 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2024-09-25 10:38:39 [scrapy.core.engine] INFO: Spider opened\n",
            "2024-09-25 10:38:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2024-09-25 10:38:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n",
            "2024-09-25 10:38:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://aplicaciones.uc3m.es/robots.txt> (referer: None)\n",
            "2024-09-25 10:38:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/robots.txt> (referer: None)\n",
            "2024-09-25 10:38:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/bachelor-degree/data-science> (referer: None)\n",
            "2024-09-25 10:38:40 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2024-09-25 10:38:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 462,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 18341,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'elapsed_time_seconds': 1.119103,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2024, 9, 25, 10, 38, 40, 447157, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 92807,\n",
            " 'httpcompression/response_count': 2,\n",
            " 'log_count/DEBUG': 6,\n",
            " 'log_count/INFO': 20,\n",
            " 'memusage/max': 149581824,\n",
            " 'memusage/startup': 149581824,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2024, 9, 25, 10, 38, 39, 328054, tzinfo=datetime.timezone.utc)}\n",
            "2024-09-25 10:38:40 [scrapy.core.engine] INFO: Spider closed (finished)\n",
            "2024-09-25 10:38:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2> (referer: None)\n",
            "2024-09-25 10:38:40 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2024-09-25 10:38:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 505,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 8089,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'elapsed_time_seconds': 1.395124,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2024, 9, 25, 10, 38, 40, 732641, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 17988,\n",
            " 'httpcompression/response_count': 2,\n",
            " 'log_count/DEBUG': 4,\n",
            " 'log_count/INFO': 13,\n",
            " 'memusage/max': 149581824,\n",
            " 'memusage/startup': 149581824,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2024, 9, 25, 10, 38, 39, 337517, tzinfo=datetime.timezone.utc)}\n",
            "2024-09-25 10:38:40 [scrapy.core.engine] INFO: Spider closed (finished)\n",
            "/usr/local/lib/python3.10/dist-packages/scrapy/spiderloader.py:49: UserWarning: There are several spiders with the same name:\n",
            "\n",
            "  WebSpider named 'web_spider' (in lab_scrapy_1_2.spiders.total_spider)\n",
            "\n",
            "  WebSpiderSpider named 'web_spider' (in lab_scrapy_1_2.spiders.web_spider)\n",
            "\n",
            "  This can cause unexpected behavior.\n",
            "  warnings.warn(\n",
            "2024-09-25 10:38:40 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: lab_scrapy_1_2)\n",
            "2024-09-25 10:38:40 [scrapy.utils.log] INFO: Versions: lxml 4.9.4.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.1, Platform Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/spiderloader.py\", line 87, in load\n",
            "    return self._spiders[spider_name]\n",
            "KeyError: 'total_spider'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/scrapy\", line 8, in <module>\n",
            "    sys.exit(execute())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/cmdline.py\", line 161, in execute\n",
            "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/cmdline.py\", line 114, in _run_print_help\n",
            "    func(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/cmdline.py\", line 169, in _run_command\n",
            "    cmd.run(args, opts)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/commands/crawl.py\", line 23, in run\n",
            "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/crawler.py\", line 264, in crawl\n",
            "    crawler = self.create_crawler(crawler_or_spidercls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/crawler.py\", line 300, in create_crawler\n",
            "    return self._create_crawler(crawler_or_spidercls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/crawler.py\", line 385, in _create_crawler\n",
            "    spidercls = self.spider_loader.load(spidercls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/spiderloader.py\", line 89, in load\n",
            "    raise KeyError(f\"Spider not found: {spider_name}\")\n",
            "KeyError: 'Spider not found: total_spider'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results.txt:**\n",
        "\n",
        "Calculus I 6 BC\n",
        "\n",
        "Introduction to Data Science 6 BC\n",
        "\n",
        "Linear algebra 6 BC\n",
        "\n",
        "Probability and Data Analysis 6 BC\n",
        "\n",
        "Programming 6 BC\n",
        "\n",
        "Programme Description:\n",
        "\n",
        "1.\tData Collection in the Web ecosystem:\n",
        "        \n",
        "        1.1  Scrapers, Crawlers\n",
        "        \n",
        "        1.2\tAPIs\n",
        "2.\tData Analytics in the web\n",
        "\n",
        "        2.1  Graph Analysis: Centrality and Influence metrics\n",
        "\n",
        "        2.2  Network structure:\n",
        "      \n",
        "        2.2.1 Type of networks (bipartite graph, small world, scale free)\n",
        "               \n",
        "      2.2.2 Clustering, Community Detection, K-core decomposition\n",
        "\n",
        "3.\tWeb data visualization\n",
        "\n",
        "        3.1\tRepresentation of web information.\n",
        "\n",
        "        3.2\tVisualization tools.\n",
        "\n",
        "4.\tFinal Web Analytics Project\n",
        "\n",
        "        4.1\tThe project needs to include the three components presented above (Data Collection, Data Analytics and Data Visualization\n",
        "\n",
        "        \n"
      ],
      "metadata": {
        "id": "toKdV1dszeA0"
      }
    }
  ]
}